# InferenceCenter

Motor de inferencia de alto rendimiento optimizado para GPUs de recursos limitados (GTX 1060 3GB), utilizando `llama.cpp` integrado directamente en C++ para mÃ¡xima eficiencia y mÃ­nima latencia.

## ğŸš€ Quick Start

### CompilaciÃ³n RÃ¡pida (CPU-Only)

```bash
# Instalar dependencias
sudo apt-get install -y build-essential cmake git zlib1g-dev

# Compilar
cd /home/sito/InferenceCenter
mkdir build && cd build
cmake -DUSE_CUDA=OFF ..
make -j$(nproc)

# Ejecutar (necesitas un modelo .gguf)
./InferenceCore /ruta/a/modelo.gguf 3000
```

### CompilaciÃ³n con GPU (Recomendado)

```bash
# 1. Instalar CUDA y drivers
sudo apt-get install -y nvidia-driver-535 nvidia-cuda-toolkit
sudo reboot

# 2. Compilar con soporte GPU
cd /home/sito/InferenceCenter
mkdir build && cd build
cmake -DUSE_CUDA=ON ..
make -j$(nproc)

# 3. Ejecutar
./InferenceCore /ruta/a/modelo.gguf 3000
```

> **ğŸ“– DocumentaciÃ³n completa**: Ver [BUILD.md](BUILD.md) para instrucciones detalladas, requisitos y soluciÃ³n de problemas.

---

## ğŸ“Š Estado Actual

### âœ… Fase 1: Hardware Monitoring (Completada)

- **Core C++ Nativo**: Wrapper robusto sobre la API de C de `llama.cpp`
- **MonitorizaciÃ³n GPU**: Clase `Monitor` con NVML para rastrear VRAM, temperatura y throttling
- **CompilaciÃ³n Dual**: Soporte para CPU-only y GPU-enabled
- **WebSocket Server**: Servidor en tiempo real con streaming de tokens
- **Protocolo JSON**: Operaciones `INFER`, `ABORT`, `TOKEN`, `END`

### âœ… Fase 2: Smart Split Computing + Metrics (Completada)

- **Auto-detecciÃ³n GPU**: Calcula automÃ¡ticamente cuÃ¡ntas capas cargar en GPU segÃºn VRAM disponible
- **Buffer de seguridad**: Reserva 500MB para prevenir OOM
- **Contexto optimizado**: Reducido a 512 tokens para conversaciones cortas
- **MÃ©tricas en tiempo real**: Broadcasting cada 1 segundo con stats de GPU e inferencia
- **OperaciÃ³n METRICS**: Nueva operaciÃ³n WebSocket para dashboards externos

> **ğŸ“– Ver**: [docs/PHASE2_WALKTHROUGH.md](docs/PHASE2_WALKTHROUGH.md) para detalles de implementaciÃ³n

### ğŸ”„ PrÃ³ximas Fases

- [ ] **Fase 3**: Watchdog (detecciÃ³n de cuelgues y auto-recuperaciÃ³n)
- [ ] **Fase 4**: Cliente Web (UI para testing y visualizaciÃ³n de mÃ©tricas)
- [ ] **Fase 5**: ProducciÃ³n (systemd, logging estructurado)

---

## ğŸ—ï¸ Arquitectura

```
src/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ Engine.cpp/.h        # Motor de inferencia (llama.cpp wrapper)
â”‚   â””â”€â”€ Allocator.h          # (Futuro) GestiÃ³n de KV Cache
â”œâ”€â”€ server/
â”‚   â”œâ”€â”€ WsServer.cpp/.h      # Servidor WebSocket (uWebSockets)
â”‚   â”œâ”€â”€ Protocol.h           # Definiciones de protocolo JSON
â”‚   â””â”€â”€ Client.h             # Estado por cliente
â”œâ”€â”€ hardware/
â”‚   â””â”€â”€ Monitor.cpp/.h       # MonitorizaciÃ³n NVML (GPU stats)
â””â”€â”€ main.cpp                 # Punto de entrada
```

---

## ğŸ”§ Opciones de CompilaciÃ³n

| Modo | Flag CMake | Requisitos | Velocidad | MonitorizaciÃ³n |
|------|------------|------------|-----------|----------------|
| **CPU-Only** | `-DUSE_CUDA=OFF` | GCC, CMake | Lenta | âŒ |
| **GPU** | `-DUSE_CUDA=ON` | CUDA, Drivers NVIDIA | 10-50x mÃ¡s rÃ¡pida | âœ… |

---

## ğŸ“¡ Protocolo WebSocket

### Cliente â†’ Servidor

```json
{
  "op": "infer",
  "prompt": "Explica la teorÃ­a de la relatividad",
  "params": {
    "temp": 0.7,
    "max_tokens": 500
  }
}
```

### Servidor â†’ Cliente (Streaming)

```json
{"op": "token", "content": " La"}
{"op": "token", "content": " teorÃ­a"}
{"op": "token", "content": " de"}
...
{
  "op": "end",
  "stats": {
    "ttft_ms": 45,
    "total_ms": 1200,
    "tokens": 87,
    "tps": 72.5
  }
}
```

### Servidor â†’ Cliente (MÃ©tricas en Tiempo Real)

Cada 1 segundo, el servidor envÃ­a automÃ¡ticamente:

```json
{
  "op": "metrics",
  "timestamp": 1706735234,
  "gpu": {
    "temp": 65,
    "vram_total_mb": 3072,
    "vram_used_mb": 1850,
    "vram_free_mb": 1222,
    "power_watts": 85,
    "fan_percent": 60,
    "throttling": false
  },
  "inference": {
    "state": "idle",
    "last_tps": 45.2,
    "last_ttft_ms": 120,
    "total_tokens_generated": 1523
  }
}
```

---

## ğŸ¯ Modelos Recomendados (GTX 1060 3GB)

| Modelo | CuantizaciÃ³n | TamaÃ±o | Rendimiento |
|--------|--------------|--------|-------------|
| Llama-3.2-1B | q4_k_m | ~800MB | âœ… Excelente |
| Llama-3.2-3B | q4_k_m | ~2.1GB | âœ… Muy bueno |
| Mistral-7B | q4_k_m | ~4.8GB | âš ï¸ Requiere split CPU/GPU |
| Llama-3-8B | q4_k_m | ~5.2GB | âš ï¸ Requiere split CPU/GPU |

Descarga modelos desde: https://huggingface.co/models?library=gguf

---

## ğŸ§ª Testing

### Test con Cliente Python

```bash
# Instalar dependencias
pip install websockets

# Ejecutar cliente de prueba
python3 test_client.py
```

### Test Manual (wscat)

```bash
# Instalar wscat
npm install -g wscat

# Conectar
wscat -c ws://localhost:3000

# Enviar prompt
{"op":"infer","prompt":"Hola, Â¿cÃ³mo estÃ¡s?"}
```

---

## ğŸ“š DocumentaciÃ³n

### GuÃ­as de Usuario
- **[BUILD.md](BUILD.md)**: GuÃ­a completa de compilaciÃ³n e instalaciÃ³n
- **[IMPLEMENTATION_PLAN.md](IMPLEMENTATION_PLAN.md)**: Plan de implementaciÃ³n original

### DocumentaciÃ³n TÃ©cnica (docs/)
- **[docs/STATUS.md](docs/STATUS.md)**: Estado actual del proyecto y opciones disponibles
- **[docs/ROADMAP.md](docs/ROADMAP.md)**: Hoja de ruta y anÃ¡lisis del proyecto
- **[docs/ROADMAP_DETAILS.md](docs/ROADMAP_DETAILS.md)**: ExplicaciÃ³n tÃ©cnica detallada de decisiones
- **[docs/PHASE2_WALKTHROUGH.md](docs/PHASE2_WALKTHROUGH.md)**: Walkthrough de implementaciÃ³n Fase 2

---

## ğŸ› SoluciÃ³n de Problemas

### "Could not find nvcc"
```bash
# Compilar sin CUDA
cmake -DUSE_CUDA=OFF ..
```

### "CUDA OOM"
Usa modelos mÃ¡s pequeÃ±os o cuantizaciones mÃ¡s agresivas (q4_k_m en lugar de q5_k_m).

### "NVML initialization failed"
```bash
# Verificar drivers
nvidia-smi

# Reinstalar si es necesario
sudo apt-get install --reinstall nvidia-driver-535
```

Ver [BUILD.md](BUILD.md) para mÃ¡s detalles.

---

## ğŸ“„ Licencia

Este proyecto utiliza `llama.cpp` (MIT License). Consulta el repositorio original para mÃ¡s detalles.

---

## ğŸ¤ Contribuciones

Este es un proyecto personal optimizado para hardware especÃ­fico (GTX 1060 3GB). Si tienes sugerencias o mejoras, abre un issue.
