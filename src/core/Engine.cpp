#include "Engine.h"
#include <iostream>
#include <vector>
#include <cstring>
#include <stdexcept>
#include <mutex>

namespace Core {

    static std::once_flag backend_init_flag;

    Engine::Engine() {
        std::call_once(backend_init_flag, []() {
            llama_backend_init();
        });
    }

    Engine::~Engine() {
        if (ctx) llama_free(ctx);
        if (model) llama_model_free(model);
    }

    bool Engine::isLoaded() const {
        return model != nullptr && ctx != nullptr;
    }

    bool Engine::loadModel(const EngineConfig& config) {
        if (isLoaded()) return false;

        // Model Parameters
        auto mparams = llama_model_default_params();
        mparams.n_gpu_layers = config.n_gpu_layers;
        mparams.use_mmap = config.use_mmap;
        mparams.use_mlock = config.use_mlock;

        // Load Model
        model = llama_model_load_from_file(config.modelPath.c_str(), mparams);
        if (!model) {
            std::cerr << "Failed to load model: " << config.modelPath << std::endl;
            return false;
        }

        // Context Parameters
        auto cparams = llama_context_default_params();
        cparams.n_ctx = config.ctx_size;
        
        // Create Context
        ctx = llama_init_from_model(model, cparams);
        if (!ctx) {
            std::cerr << "Failed to create context" << std::endl;
            llama_model_free(model);
            model = nullptr;
            return false;
        }

        return true;
    }

    std::vector<llama_token> Engine::tokenize(const std::string & text, bool add_bos) {
        // Upper limit for the number of tokens
        int n_tokens_max = text.length() + (add_bos ? 1 : 0) + 1;
        std::vector<llama_token> tokens(n_tokens_max);
        
        const llama_vocab* vocab = llama_model_get_vocab(model);

        int n_tokens = llama_tokenize(vocab, text.c_str(), text.length(), tokens.data(), n_tokens_max, add_bos, false);
        
        if (n_tokens < 0) {
            tokens.resize(-n_tokens);
            n_tokens = llama_tokenize(vocab, text.c_str(), text.length(), tokens.data(), tokens.size(), add_bos, false);
        }

        if (n_tokens >= 0) {
            tokens.resize(n_tokens);
        } else {
            // Should not happen if logic is correct
            tokens.clear();
        }
        
        return tokens;
    }

    std::string Engine::tokenToPiece(llama_token token) {
        if (!model) return "";
        char buf[256];
        const llama_vocab* vocab = llama_model_get_vocab(model);
        int n = llama_token_to_piece(vocab, token, buf, sizeof(buf), 0, true); // 0=no special handling, true=render special
        if (n < 0) {
            // Buffer too small?
            return ""; 
        }
        return std::string(buf, n);
    }

    Metrics Engine::generate(const std::string& prompt, TokenCallback callback) {
        Metrics metrics;
        if (!isLoaded()) return metrics;

        llama_memory_clear(llama_get_memory(ctx), false);

        auto start_time = std::chrono::high_resolution_clock::now();
        bool is_first_token = true;

        // 1. Tokenize
        std::vector<llama_token> tokens_list = tokenize(prompt, true);

        // 2. Prepare Sampler
        auto sparams = llama_sampler_chain_default_params();
        struct llama_sampler * smpl = llama_sampler_chain_init(sparams);
        llama_sampler_chain_add(smpl, llama_sampler_init_greedy());

        // 3. Prepare Batch
        llama_batch batch = llama_batch_init(std::max((int)tokens_list.size(), 1), 0, 1); 

        // Load prompt
        batch.n_tokens = 0;
        for (size_t i = 0; i < tokens_list.size(); i++) {
             batch.token[i] = tokens_list[i];
             batch.pos[i] = i;
             batch.n_seq_id[i] = 1;
             batch.seq_id[i][0] = 0;
             batch.logits[i] = false;
             batch.n_tokens++;
        }
        
        // precise logits for the last token
        if (batch.n_tokens > 0) {
            batch.logits[batch.n_tokens - 1] = true;
        }

        if (llama_decode(ctx, batch) != 0) {
            std::cerr << "llama_decode failed" << std::endl;
            llama_batch_free(batch);
            llama_sampler_free(smpl);
            return metrics;
        }

        // 4. Generation Loop
        int n_cur = batch.n_tokens;
        const llama_vocab* vocab = llama_model_get_vocab(model);

        while (true) { 
            // Sample
            llama_token new_token_id = llama_sampler_sample(smpl, ctx, -1);
            
            // Accept/Callbacks
            llama_sampler_accept(smpl, new_token_id);

            // Time to First Token
            if (is_first_token) {
                auto now = std::chrono::high_resolution_clock::now();
                metrics.ttft_ms = std::chrono::duration_cast<std::chrono::milliseconds>(now - start_time).count();
                is_first_token = false;
            }

            if (llama_vocab_is_eog(vocab, new_token_id)) {
                break;
            }

            std::string piece = tokenToPiece(new_token_id);
            metrics.tokens_generated++;

            if (callback) {
                 if (!callback(piece)) break; // User aborted
            }

            // Prepare next batch for single token
            batch.n_tokens = 0;
            
            batch.token[0] = new_token_id;
            batch.pos[0] = n_cur;
            batch.n_seq_id[0] = 1;
            batch.seq_id[0][0] = 0;
            batch.logits[0] = true;
            batch.n_tokens = 1;
            
            n_cur++;

            if (llama_decode(ctx, batch) != 0) {
                std::cerr << "llama_decode failed during generation" << std::endl;
                break;
            }
        }
        
        // Finalize Metrics
        auto end_time = std::chrono::high_resolution_clock::now();
        metrics.total_time_ms = std::chrono::duration_cast<std::chrono::milliseconds>(end_time - start_time).count();
        if (metrics.total_time_ms > 0) {
            metrics.tps = (double)metrics.tokens_generated / (metrics.total_time_ms / 1000.0);
        }

        // Cleanup
        llama_batch_free(batch);
        llama_sampler_free(smpl);
        
        return metrics;
    }
    
    std::string Engine::getSystemInfo() const {
        return llama_print_system_info();
    }

}
